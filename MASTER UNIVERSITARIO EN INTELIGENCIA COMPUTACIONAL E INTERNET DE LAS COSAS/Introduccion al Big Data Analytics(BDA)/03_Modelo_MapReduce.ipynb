{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Modelo de programación MapReduce (Python)</b>\n",
        "## <i>Big Data Analytics</i>\n",
        "\n",
        "Curso 2022/23\n",
        "\n",
        "Prof. *Dr. José Raúl Romero Salguero*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LUXZZJ93Ftqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver en este notebook una introducción al desarrollo con el modelo de programación MapReduce en Python.\n",
        "\n",
        "Al desarrollarse sobre Google Colab, no se implementa para un entorno clúster, si bien sí que se respetará la filosofía del paradigma.\n",
        "\n",
        "Para conocer el modelo, tras la instalación del entorno, comenzaremos definiendo las funciones `map` y `reduce` de Python.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UrAEyNEG3w5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalación del entorno**\n",
        "## Instalación de Hadoop\n",
        "\n",
        "Instalamos la versión de Hadoop/Spark 3.2.3\n",
        "Se recomienda visitar el sitio de Apache Spark para descargar la última versión estable:\n",
        "\n",
        "https://spark.apache.org/downloads.html\n",
        "\n",
        "Se configuran posteriormente las variables de entorno `JAVA_HOME` y `SPARK_HOME`"
      ],
      "metadata": {
        "id": "oA8Fsl8E2YSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "iDM_FjfutRUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La descarga de Hadoop puede tomar su tiempo, según la conexión disponible. Se borra posteriormente de la máquina virtual el archivo `.tgz`"
      ],
      "metadata": {
        "id": "ygAAV5Hc3go6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gboXJnmYsHJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5a145b-5831-4d8c-c798-543ba084faa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-18 09:55:59--  https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 301136158 (287M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.3-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.2.3-bin-had 100%[===================>] 287.19M   209MB/s    in 1.4s    \n",
            "\n",
            "2023-01-18 09:56:01 (209 MB/s) - ‘spark-3.2.3-bin-hadoop3.2.tgz’ saved [301136158/301136158]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Descomentar las líneaa según la necesidad\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar -xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "#!rm spark-3.2.3-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación e iniciación de la sesión de Spark\n",
        "\n",
        "* Buscamos la librería `findspark` con `pip install`\n"
      ],
      "metadata": {
        "id": "gc1G4c_l4JzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "ouepVeD-Hg30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Con `SparkSession` inicializamos"
      ],
      "metadata": {
        "id": "wgNshvyOHhfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Spark_Dataframes\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "jXPu1jn_tYoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "TmZVwuoCJ6pe",
        "outputId": "91e6acef-c95a-4ac3-9fb1-0911c33d5628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f82f7dd67c0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://edbc25339155:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark_Dataframes</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***MapReduce*** en Python\n",
        "\n",
        "* *MapReduce* es un **modelo de programación en paralelo escalable**, esto es, concebido para operar con grandes volúmenes de datos en grandes clústers.\n",
        "* La **infraestructura**/plataforma (p.ej. Apache Hadoop, Apache Spark, etc.) se encarga de ocultar al programador operaciones de los sistemas distribuidos como el balanceo de carga, tráfico en red, optimización de transferencia en disco, serialización de los datos, gestión de fallos de máquinas/transferencia de datos, etc.\n",
        "  * Hadoop está escrito en Java, y Spark en Scala\n",
        "  * Hadoop permite ser desarrollado en otros lenguajes, como Python, para lo que se debe transformar el código en un `JAR` de Java, p.ej. utilizando [Jython](https://www.jython.org/).\n",
        "  * Spark ofrece interfaz/librería para el uso de Python (pyspark [texto del enlace](https://spark.apache.org/docs/latest/api/python/))\n",
        "* La **programación funcional** de Python, vista en el notebook anterior, encaja muy bien en el modelo de programación, si bien no es de uso obligado.\n"
      ],
      "metadata": {
        "id": "8labSFrG42Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Recordemos** los pasos habituales del modelos de programación *MapReduce*\n",
        "\n",
        "1. Extracción, transformación y carga de un gran conjunto de datos.\n",
        "2. Operación `map`\n",
        "3. Combinar (*shuffle*) y ordenar → localización de tareas en nodos\n",
        "4. Operación `reduce` (resumen, filtrado y transformación)\n",
        "5. Escritura de resultados\n",
        "\n",
        "Las funciones `map` y `reduce` pueden trabajar en paralelo sobre distintas claves o distintos elementos de la colección de datos. "
      ],
      "metadata": {
        "id": "LXvwQXK45E4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de conteo de palabras (*Word count*)\n",
        "\n",
        "Es un ejemplo ilustrativo muy utilizado por Apache Foundation para explicar el funcionamiento de MapReduce. El **problema** consiste en contar cada una de las palabras que aparecen en un conjunto de documentos (identificados por su URI)."
      ],
      "metadata": {
        "id": "LHuYSNBFSTwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, debemos **formalizar el formato de las tuplas** `<clave, valor>` con las que trabajará la función `map`.\n",
        "\n",
        "* key: URI\n",
        "* value: Contenido del documento\n",
        "\n",
        "$\\left< shakesp1, to \\; be \\; or \\; not \\; to \\; be \\right>$ "
      ],
      "metadata": {
        "id": "1H7S-YoUTzWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En segundo lugar, la **función `map`** obtendrá los pares de conteo individual de cada una de las palabras del conjunto de datos que recibe ese `map`.\n",
        "\n",
        ">  $\\left< to, 1 \\right>$\n",
        "  $\\left< be, 1 \\right>$\n",
        "  $\\left< or, 1 \\right>$\n",
        "  $\\left< not, 1 \\right>$\n",
        "  $\\left< to, 1 \\right>$\n",
        "  $\\left< be, 1 \\right>$"
      ],
      "metadata": {
        "id": "TNbxeewxUqzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En tercer lugar, dada la simplicidad de la **operación `reduce`**, es la función reduce la que agrupa los valores conforme a su clave y realiza la suma.\n",
        "\n",
        ">  $\\left< be, 2 \\right>$  \n",
        "  $\\left< not, 1 \\right>$  \n",
        "  $\\left< or, 1 \\right>$  \n",
        "  $\\left< to, 2 \\right>$  \n"
      ],
      "metadata": {
        "id": "3xJa217jVEBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El **procedimiento general** a seguir para cualquier problema: \n",
        "\n",
        "- La función `map` agrupa los valores según su *`key`*, y luego invoca a un `reduce` para cada clave.\n",
        "\n",
        "  - Para ello, las colecciones se dividen en diferentes unidades de almacenamiento.\n",
        "\n",
        "  - MapReduce particionará los datos minimizando la copia de estos en el clúster.\n",
        "\n",
        "- Los datos de las diferentes particiones se reducen por separado y en paralelo.\n",
        "\n",
        "- El resultado final de la función `reduce` es una reducción de los datos ya reducidos en cada partición individual.\n",
        "\n",
        "  - Recordemos que, para que funcione, **el operador debe ser conmutativo y asociativo**.\n",
        "\n",
        "  - En el caso de esta función, el operador es `+`.\n",
        "\n",
        "---\n",
        "\n",
        "**NOTA**: A diferencia de `map` / `reduce` de Python estándar, en este caso la función `reduce` funciona exclusivamente con pares $\\left< key, value\\right>$. Algunos autores, como Phelps proponen nombrarla **`ReduceByKey`** (`reducirPorClave`)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "espkT5jQXUEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementación no escalable, no paralela\n",
        "\n",
        "Tomada de (Phelps, 2016), desarrollamos un *ejemplo no paralelo y no escalable* del problema planteado. Por tanto, es importante considerar que **esta no es la forma** en la que Hadoop o Spark lo implementan. El **objetivo** es mostrar cómo funciona el modelo de programación MapReduce y cómo un ejemplo sencillo puede ser codificado en términos de operaciones `map` y `reduce`."
      ],
      "metadata": {
        "id": "n4RPFtPJY0Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, codificamos las funciones de combinar (agrupar por clave - *`groupByKey`* o `agruparPorClave`) y reducir por clave (*`reduceByKey`* o `reducirPorClave`):"
      ],
      "metadata": {
        "id": "_wHtQe62ZrZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "def agruparPorClave(data):\n",
        "  # Devolvemos un conjunto de pares <clave, valor(es)>\n",
        "  res = dict()\n",
        "  # Para cada <clave, valor> en los datos (previsiblemente de map)\n",
        "  for clave, valor in data:\n",
        "    if clave in res:\n",
        "      # Si la clave ya existe en el diccionario, se agrega otro valor a esa misma clave\n",
        "      res[clave].append(valor)\n",
        "    else:\n",
        "      # Si no existe, se crea nueva\n",
        "      res[clave] = [valor]\n",
        "  # Se devuelve la nueva agrupación\n",
        "  return res\n",
        "\n",
        "def reducirPorClave(fn, data):\n",
        "  # Lo primero para reduce es agrupar por la misma clave\n",
        "  pares = agruparPorClave(data)\n",
        "  # Como reducción, se devolverá una colección de pares <clave, valor'>, donde\n",
        "  # valor' es el resultado de aplicar fn a la lista de valores con esa clave.\n",
        "  return list(map(lambda clave: \n",
        "                  (clave, reduce(fn, pares[clave])), \n",
        "                  pares))"
      ],
      "metadata": {
        "id": "FBIk2YEbaduM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos particularizar el código anterior aplicándolo a un caso concreto del problema *WordCount*. Continuamos con el ejemplo que se inició arriba:"
      ],
      "metadata": {
        "id": "xvL85CTOcSTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La función de mapeo consiste en generar pares (x, 1) para cada palabra que se encuentra en el conjunto de datos\n",
        "data = list(map(lambda x: (x, 1), \"to be or not to be\".split()))\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm81B2MWcfPZ",
        "outputId": "145ff4ef-06de-44da-fb7a-d0eb2bdda946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('to', 1), ('be', 1), ('or', 1), ('not', 1), ('to', 1), ('be', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estos pares deben ser agrupados por clave\n",
        "agruparPorClave(data)\n",
        "# NOTA: Esta llamada es solo a modo ilustrativo (por eso no se asigna a nada), ya que se llama en la siguiente invocación de la operación reduce()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K0TVdRmeBWw",
        "outputId": "289465f7-f0fc-46d7-de21-cebc1cdced72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': [1, 1], 'be': [1, 1], 'or': [1], 'not': [1]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reducirPorClave(lambda x,y: x + y, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCTz1jkXepwp",
        "outputId": "efc1c3ca-3723-415f-8b60-c4ebcd96453e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementación paralela basada en hilos (no Spark)\n",
        "\n",
        "La implementación paralela se basa en la codificación `fn_map_multihilo` que se explicó en el *notebook 02*. \n",
        "\n",
        "Esta sección expone cómo realizar computaciones MapReduce que exploten el paralelismo que ofrecen los múltiples *cores* de una única computadora. Para ello, primero rescatamos la definición de `fn_map_multihilo`:"
      ],
      "metadata": {
        "id": "WG_UOhGbfxtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CÓDIGO EXPLICADO EN Notebook 02\n",
        "from threading import Thread\n",
        "\n",
        "# Descomentar print para hacer seguimiento de la ejecución o eliminar comentarios print para limpiar código\n",
        "\n",
        "def hacer_hilo(fn, res, data, hilos, i):    \n",
        "    # La evaluación de cada función se planifica en un core distinto\n",
        "    def trabajo(): \n",
        "        #print(\"trabajo(\", threading.current_thread().name, \"): Procesando datos:\", data[i])\n",
        "        # Es realmente en este punto donde se aplica lambda sobre los datos concretos del hilo\n",
        "        res[i] = fn(data[i])\n",
        "        #print(\"trabajo(\", threading.current_thread().name, \"): Finalizado hilo #\", i)    \n",
        "        #print(\"trabajo(\", threading.current_thread().name, \"): Resultado es\", res[i])\n",
        "    hilos[i] = Thread(target=trabajo)\n",
        "  \n",
        "def fn_map_multihilo(fn, data):\n",
        "    # El programa principal es el encargado de planificar los trabajos, ejecutar los hilos y esperar a que terminen\n",
        "    n = len(data)\n",
        "    res = [None] * n\n",
        "    hilos = [None] * n\n",
        "    #print(\"fn_map_multihilo(): Planificando trabajos\")\n",
        "    for i in range(n):\n",
        "        hacer_hilo(fn, res, data, hilos, i)\n",
        "    #print(\"fn_map_multihilo(): Iniciando trabajos\")\n",
        "    for i in range(n):\n",
        "        hilos[i].start()\n",
        "    #print(\"fn_map_multihilo(): Esperando terminación de hilos\")\n",
        "    for i in range(n):\n",
        "        hilos[i].join()\n",
        "    #print(\"¡Terminado!\")\n",
        "    return res"
      ],
      "metadata": {
        "id": "zwk6wLTqf_l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la función de mapeo multihilo ya definida, lo siguiente es hacer uso de ella en la versión anterior de `reducirPorClave`. Recuerda que `fn_map_multihilo` no se refiere a la función MapReduce, sino a la operación Python de programación funcional.\n",
        "\n",
        "Redefinimos `reducirPorClave`:"
      ],
      "metadata": {
        "id": "uelbncdyh9Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reducirPorClave_multihilo(fn, data):\n",
        "  pares = agruparPorClave(data)\n",
        "  return fn_map_multihilo(lambda clave: (clave, reduce(fn, pares[clave])), [clave for clave in pares])"
      ],
      "metadata": {
        "id": "eshEhIItjM9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**NOTA**: En Python, `[expr for loop]` es la sintaxis para la comprensión de listas (***list comprehension***), que permite crear un lista en una única línea de código. Esta es una de las características distintivas más potentes de Python, pero debe utilizarse con cuidado por motivos de legibilidad y eficiencia.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "> `cuadrados = [i * i for i in range(10)]`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wrGw9iyykvG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuamos con el ejemplo multihilo aplicado al conteo de palabras. Para ello, invocamos a `reducirPorClave_multihilo` con la función de suma y los datos de Shakespeare."
      ],
      "metadata": {
        "id": "cp1IzP9zmIkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reducirPorClave_multihilo(lambda x,y: x + y, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu0Gef3Fmdt6",
        "outputId": "50c59fc0-7dcf-4d23-ed60-e43925264feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero si nos fijamos en el código de `reducirPorClave_multihilo`, el paso `reduce` de MapReduce no está realmente siendo paralelizado en términos de la distribución de datos que aplicaría MapReduce. Realmente, para **paralelizar la tarea `reduce`** debemos considerar:\n",
        "\n",
        "- El operador debe ser conmutatitvo y asociativo.\n",
        "- Los datos deben estar divididos en particiones de tamaño similar.\n",
        "- Aplicamos la operación reduce a cada partición independientemente en un core separado.\n",
        "- Los resultados se combinan al final del paso de reducción."
      ],
      "metadata": {
        "id": "Un5Voc6Cn8tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, implementamos la operación de partición de datos:"
      ],
      "metadata": {
        "id": "vAMlBOEep0hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def particionar_datos(data, ptos_particion):\n",
        "    # Declaramos la lista de particiones\n",
        "    particiones = []\n",
        "    # Número del primer elemento de la siguiente partición \n",
        "    n = 0\n",
        "    for i in ptos_particion:\n",
        "        # Agregamos a particiones subconjuntos del dataset\n",
        "        particiones.append(data[n:i])\n",
        "        n = i\n",
        "    # Agregamos el último subconjunto\n",
        "    particiones.append(data[n:])\n",
        "    return particiones"
      ],
      "metadata": {
        "id": "7Dgh5jgnp8BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En segundo lugar, creamos la función `reduce` para operar sobre múltiples particiones de datos de forma paralela:"
      ],
      "metadata": {
        "id": "lMnZtsaoqrhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_paralelo(fn, particiones):\n",
        "  num_part = len(particiones)\n",
        "  res = [None] * num_part\n",
        "  hilos = [None] * num_part\n",
        "\n",
        "  def trabajo(i):\n",
        "    res[i] = reduce(fn, particiones[i])\n",
        "  \n",
        "  for i in range(num_part):\n",
        "    hilos[i] = Thread(target = lambda: trabajo(i))\n",
        "    hilos[i].start()\n",
        "\n",
        "  for i in range(num_part):\n",
        "    hilos[i].join()\n",
        " \n",
        "  print(\"Reducciones de hilos: \",res)\n",
        "  return reduce(fn, res)"
      ],
      "metadata": {
        "id": "vl5x031hq5HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos un conjunto de datos que queremos fusionar en una única palabra:\n",
        "datos = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n",
        "# Que queremos particionar en 3 subconjuntos: a partir del 3er y 6o elemento\n",
        "particiones = particionar_datos(datos, [2,5])\n",
        "print(\"Particiones de datos: \", particiones)\n",
        "# La reducción se hace por particiones de forma paralela y posteriormente se fusionan los resultados de las 3 particiones:\n",
        "reduce_paralelo(lambda x,y: x + y, particiones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "TJ-uIMLrvTkR",
        "outputId": "a94228c2-3312-4fb0-decb-6f210f8382e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Particiones de datos:  [['a', 'b'], ['c', 'd', 'e'], ['f', 'g']]\n",
            "Reducciones de hilos:  ['ab', 'cde', 'fg']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcdefg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***MapReduce*** en Spark\n",
        "\n",
        "Recordemos que Spark es un superconjunto de MapReduce, extensión de Hadoop, también de Apache. \n",
        "\n",
        "Provee objetos que representan los RDD (***Resilient Distributed Datasets***) \n",
        "\n",
        "> Un RDD es la estructura de datos básica fundamental de Spark. Hay colecciones de datos distribuidos inmutables de cualquier tipado, constituyendo registros de datos tolerantes a fallos (resilientes) que residen en múltiples nodos.\n",
        "\n",
        "**¿Cómo se comportan los RDD?** De forma similar a las listas de Python pero considerando que las colecciones son inmutables y que los datos se distribuyen por los nodos del clúster.\n",
        "\n",
        "Al aplicar MapReduce, cada instancia de un RDD tiene declarados al menos dos métodos para el flujo de tareas: \n",
        "\n",
        "- `map`\n",
        "- `reduce` (→ `reducirPorClave`)\n",
        "\n",
        "El funcionamiento de estos métodos es el mismo que el declarado arriba (solo que se hizo sobre colecciones estándares de Python)."
      ],
      "metadata": {
        "id": "vLadjQjmxV-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversión a RDD con el contexto de Spark: clase `SparkContext`\n",
        "\n",
        "Cuando se trabaja con Spark, invocaremos a métodos de un objeto de `pyspark.context.SparkContext`, que representa al contexto de ejecución. Un nombre habitualmente utilizado para estas instancias es **`sc`**.\n",
        "\n",
        "El **método `parallelize`** se utiliza para convertir una colección estándar de Python en un RDD. Lo más habitual es que este RDD se cree a partir de una tabla HBase o de un gran dataset.\n",
        "\n",
        "Para ello, una vez inicializado el entorno, se crea el contexto de Spark."
      ],
      "metadata": {
        "id": "FraVrh-E5OHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc =SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "5aGsJIH_6xH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supongamos que queremos convertir una lista de Python en un RDD."
      ],
      "metadata": {
        "id": "YCFf2QXw60zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palabras = \"to be or not to be\".split()\n",
        "palabras_rdd = sc.parallelize(palabras)\n",
        "palabras_rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mn0B0Sn68Oc",
        "outputId": "b403d7a3-4f31-4007-d99d-10a991d5c45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obsérvese que con la invocación a `map` o `reducirPorClave` sobre un RDD (p.ej. `palabras_rdd`) ya podemos configurar una operación de procesamiento paralelo distribuido por el clúster.\n"
      ],
      "metadata": {
        "id": "0x4iCKTyq3Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función `map` sobre un RDD\n",
        "\n"
      ],
      "metadata": {
        "id": "gURNOiZz7KGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En Spark, los RDD implementan los métodos de `map` y `reduceByKey`."
      ],
      "metadata": {
        "id": "0QT3gIpOmPia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuplas_palabras_rdd = palabras_rdd.map(lambda x: (x, 1))\n",
        "tuplas_palabras_rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzxj5e_FrM-9",
        "outputId": "98abb25d-3361-43ea-9b7b-60280e217510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[1] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La operación anterior es lanzada en el clúster pero no se llevará a cabo la computación solicitada y, consecuentemente, no tendremos el resultado todavía hasta que solicitemos el resultado final invocando al **`método collect()`**."
      ],
      "metadata": {
        "id": "mfgPwg94sHsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuplas_palabras_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed97td-dsiEZ",
        "outputId": "fbdae2d3-9a50-48d3-9cef-ab8cdaff8bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 1), ('be', 1), ('or', 1), ('not', 1), ('to', 1), ('be', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recuerda**:\n",
        "- Solo cuando invocamos a `collect` se realiza el procesamiento en el clúster.\n",
        "- Si la colección resultante del mapeo es muy grande, entonces puede ser una operación muy costosa.\n",
        "\n",
        "En caso de que sea muy costoso y queramos hacer pruebas o no nos interese el resultado completo por algún motivo, la **operación `take`** es similar a `collect` pero devuelve únicamente los primeros `n` elementos."
      ],
      "metadata": {
        "id": "QpudptrXt4rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuplas_palabras_rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXi1k39mt6X8",
        "outputId": "fb9bb57e-cc9b-469f-cc4f-c848f71b5149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 1), ('be', 1), ('or', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función `reduce` sobre un RDD\n",
        "\n",
        "Una vez realizada la primera tarea de mapeo, necesitamos ejecutar las siguientes tareas del trabajo MapReduce. Procedemos de forma similar al mapeo con la reducción por clave. Para ello, utilizamos el **método `reduceByKey`**, cuyo funcionamiento interno hemos simulado previamente en los ejemplos anteriores."
      ],
      "metadata": {
        "id": "ZkciY0-rsuTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuplas_palabras_rdd = tuplas_palabras_rdd.reduceByKey(lambda x,y: x + y)"
      ],
      "metadata": {
        "id": "7GKGaNjcuW27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y solicitamos que se compute el resultado final:"
      ],
      "metadata": {
        "id": "Zi4g1iaKu5p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuplas_palabras = tuplas_palabras_rdd.collect()\n",
        "tuplas_palabras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ_XCIXIu84V",
        "outputId": "11f90afd-f14d-4a41-9fdc-c167448cd75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de conteo de palabras en MapReduce\n",
        "\n",
        "Todo lo anterior se resume en que podemos implementar todas las tareas de MapReduce para el caso del conteo de palabras simplemente con el siguiente código (Phelps, 2016):"
      ],
      "metadata": {
        "id": "DR98Ngyy1_0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"to be or not to be\".split()\n",
        "rdd = sc.parallelize(text)\n",
        "counts = rdd.map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda x, y: x + y)\n",
        "counts.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR4h_0s12QTT",
        "outputId": "092fd85e-16b0-4082-9d33-0b1f1791aefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operaciones alternativas sobre un RDD\n",
        "\n",
        "Anteriormente hemos visto como realizar el mapeo y reducción sobre clúster con un RDD. Sin embargo, en trabajos más complejos podemos necesitar otras operaciones adicionales que ayuden a la operación sobre colecciones de tuplas (propias del marco MapReduce)."
      ],
      "metadata": {
        "id": "_3_rKBWavcdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una operación alternativa es la **lectura del dataset desde un fichero de texto**, ya que no siempre disponemos del dataset en alguna de las formas que hemos visto hasta ahora - habitualmente basadas en colecciones de memoria. Para ello, utilizaremos el **método textFile**, que toma como argumento la URI del fichero, esto es, tanto un directorio local como un directorio remoto (\"`hdfs://`\", \"`s3a://`\", etc.). Además, el método `textFile` puede manejar directorios, comodines (\"`directorio/*.txt`\") e incluso ficheros comprimidos (\"`genome.txt.gz`\"). \n",
        "\n",
        "No obstante, consideremos que la forma más habitual en Big Data es que los datos se encuentren en ficheros HDFS o en tablas HBase.\n",
        "\n",
        "***Ejemplo***: Deseamos convertir el contenido de un fichero texto, *genome.txt* (sistema de ficheros ext4), en un RDD, donde cada elemento se corresponde con una línea del mismo.\n",
        "\n",
        "*NOTA*: Descargue el fichero `genome.txt` de Moodle y súbalo a su carpeta raíz del entorno actual de ejecución de Google Colab."
      ],
      "metadata": {
        "id": "EaTDTPB5v8sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genoma_rdd = sc.textFile(\"genome.txt\")\n",
        "genoma_rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TYiU4GaxFOz",
        "outputId": "5b73a6b6-8a39-48e8-b3d7-3c2dfd9d5a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TTGGCCATGCTGCCCACTCACCTAGAGCGCACAGCTGACACTGAGTCCTCTTCTGAACCTCATCCATGAA',\n",
              " 'CATATTTATGAAATCTTTCCTGGCCCCAAGTGGAAATGCCCCCTCATTTGGGTCCTCACTGAACCCCAGT',\n",
              " 'ACACAACTCTTTTGTACTACTCTATTATGCTGGGGTGTTTTTTTATTGTCTCACCTGATAAACCGTAAGC']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Otras operaciones adicionales** que podremos necesitar, según el caso, son las siguientes:\n",
        "\n",
        "- `filter(fn)` → Devuelve un nuevo dataset formado por la selección de aquellos elementos de la fuente en los que `fn` devuelve `True`.\n",
        "- `sortByKey([ascending], [numPartitions])` → Cuando se invoca a un dataset de pares (K, V), donde K implementa `Ordered`, este método devuelve un conjunto de pares (K, V) ordenados por las claves en orden ascendente o descendente, según su argumento.\n",
        "- `coalesce(numPartitions)` → Reduce el número de particiones de un RDD a `numPartitions`. Este método es especialmente útil cuando se ha realizado un filtrado y se ha reducido el tamaño de las particiones, por lo que se pueden transformar en menos para su operatividad más eficiente en el clúster.\n",
        "- `count()` → Devuelve el número de elementos del dataset.\n",
        "- `countByKey()` → Devuelve un *hashmap* de pares `(K, int)` con la cuenta de elementos asociados a cada clave `K`, sólo si es un RDD de tipo `(K, V)`. \n",
        "- `saveAsTextFile(path)` → Escribe los elementos en un fichero de texto (o conjunto de ficheros de texto). \n",
        "\n",
        "**NOTA**: La [guía de programación de RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html) de la web oficial de Spark contiene el listado completo de operaciones sobre estos elementos. "
      ],
      "metadata": {
        "id": "6BZcsW5KxuPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Ejemplo de uso de RDD*: Problema del genoma\n",
        "\n",
        "Se pretende calcular -haciendo uso de RDD- la frecuencia de secuencias de 5 bases en una cadena de genoma. "
      ],
      "metadata": {
        "id": "QR0EBEVOzC_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, se crea el método que permite agrupar y separar en grupos de *n* elementos la cadena de genoma. "
      ],
      "metadata": {
        "id": "YqiZaVubYOsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agruparCaracteres(linea, n=5):\n",
        "    res = ''\n",
        "    i = 0\n",
        "    for c in linea:\n",
        "        res = res + c\n",
        "        i = i + 1\n",
        "        if (i % n) == 0:\n",
        "            yield res\n",
        "            res = ''\n",
        "\n",
        "def agrupar_partir(linea):\n",
        "    return [sec for sec in agruparCaracteres(linea)]"
      ],
      "metadata": {
        "id": "Pi-JcksXZBW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, dado que el RDD es una colección compuesta de un elemento por línea, si utilizamos `map` devolvería un tipo de dato multidimensional, esto es, una lista por cada colección/línea leída del fichero. Por ello, tenemos que aplanar la estructura resultante del mapeado con el método `flatMap`.\n",
        "\n",
        "Posteriormente, esta lista de bases (composición de 5 caracteres) deben transformarse en un RDD que contiene la forma $\\left< K, V \\right>$, donde la clave $K$ es la secuencia y el valor $V$ es `1`. Si recordamos, en el problema del conteo de palabras se operaba igual."
      ],
      "metadata": {
        "id": "rInulPh7a1tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "secuencias = genoma_rdd.flatMap(agrupar_partir)"
      ],
      "metadata": {
        "id": "0qNmr9KEZaxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primero, convertimos la secuencia en un conjunto de <K,V> en forma MapReduce\n",
        "# Segundo, aplicamos la reducción por clave: suma de los valores individuales para la misma clave\n",
        "conteo = secuencias.map(lambda k: (k, 1)).reduceByKey(lambda x,y: x + y)\n",
        "conteo.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Gcp149cHqU",
        "outputId": "9d4b7b8e-097d-48e3-bdf2-b7b9e5d12160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('TTGGC', 587),\n",
              " ('CATGC', 647),\n",
              " ('TGCCC', 599),\n",
              " ('ACTCA', 775),\n",
              " ('TGACA', 831),\n",
              " ('TTCTG', 1257),\n",
              " ('AACCT', 726),\n",
              " ('TTATG', 819),\n",
              " ('AAATC', 996),\n",
              " ('TGGCC', 718)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para obtener los pares ordenados por el número de apariciones, podemos utilizar métodos nativos de RDD, como `sortByKey`. Puesto que en el par $\\left< K, V \\right>$ el conteo es el valor $V$, y no la clave $K$, El detalle está en que debemos revertir la clave y el valor para que ordene por este valor.\n",
        "\n",
        "Para ello, desarrollamos una función que revierta clave y valor, y un mapeo que lo aplique. Finalmente, se podrá ordenar por la clave."
      ],
      "metadata": {
        "id": "BWXsuFI7dqKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def revertir_tupla(par):\n",
        "  return (par[1], par[0])\n",
        "\n",
        "# Se revierte la clave y el valor para poder ordenar por clave\n",
        "secuencias = conteo.map(revertir_tupla)\n",
        "\n",
        "# Se ordena por clave (descendiente: False)\n",
        "secuencias_ord = secuencias.sortByKey(False)\n",
        "secuencias_ord.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzb9Cjspeu8m",
        "outputId": "a450ac05-eb1b-48da-9db9-d70387cea710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(37137, 'NNNNN'),\n",
              " (4653, 'AAAAA'),\n",
              " (4223, 'TTTTT'),\n",
              " (2788, 'AAAAT'),\n",
              " (2658, 'ATTTT')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos que nos piden la **frecuencia de aparición**. Por tanto, necesitaremos calcular todo el conteo de bases en las distintas secuencias. Si bien hay otros métodos de obtenerlos (*ver apuntes de teoría del tema*), podremos sumar las claves:"
      ],
      "metadata": {
        "id": "HxP12S1ggfk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponemos que no necesitamos más operaciones considerando la base como K. \n",
        "#     En este caso, habría que revertir el orden de nuevo (operación costosa)\n",
        "\n",
        "# Primero, calculamos el total de bases\n",
        "total = secuencias_ord.keys().sum()\n",
        "\n",
        "# Aplicamos el cálculo de la frecuencia con comprensión de lista (list comprehension)\n",
        "secuencias = [(sec[0]/total,sec[1]) for sec in secuencias_ord.collect()]\n",
        "\n",
        "print(\"Número total de bases: \",total)\n",
        "secuencias[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cPfFDA9gumX",
        "outputId": "4e5810b5-b381-4638-ecdd-c91fffbc6318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de bases:  699988\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.05305376663599948, 'NNNNN'),\n",
              " (0.006647256810116745, 'AAAAA'),\n",
              " (0.006032960565038258, 'TTTTT'),\n",
              " (0.003982925421578656, 'AAAAT'),\n",
              " (0.0037972079521363224, 'ATTTT'),\n",
              " (0.00326148448259113, 'AAATA'),\n",
              " (0.00325148431115962, 'TAAAA'),\n",
              " (0.003138625233575433, 'TTTTA'),\n",
              " (0.0031371966376566454, 'TATTT'),\n",
              " (0.0031214820825499865, 'AGAAA')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finalización de la sesión de Spark"
      ],
      "metadata": {
        "id": "0SY5QHlJHWPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "PezbLTkmHRry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Referencias</b>\n",
        "\n",
        "Estructura y contenido básico inicial adoptado de: \n",
        "* S. Phelps, [*Data science and big data with Python*](https://github.com/phelps-sg/python-bigdata), 2016\n",
        "\n",
        "Información adicional:\n",
        "\n",
        "* Ejemplo [`WordCount` en Python](https://cwiki.apache.org/confluence/display/HADOOP2/PythonWordCount) (apache.org, 2019)\n",
        "* V. Yordanov, [*Python Basics: Mutable vs Immutable Objects*](https://towardsdatascience.com/https-towardsdatascience-com-python-basics-mutable-vs-immutable-objects-829a0cb1530a#:~:text=Some%20of%20the%20mutable%20data,string%2C%20tuple%2C%20and%20range.), 2019 \n",
        "* C. Gaur, [*A Complete Guide to RDD in Apache Spark*](https://www.xenonstack.com/blog/rdd-in-spark/#:~:text=Resilient%20Distributed%20Dataset%20(RDD)%20is,that%20resides%20on%20multiple%20nodes.), 2020\n",
        "* Tutorial básico/guía de referencia de Python: https://www.w3schools.com/python/\n",
        "* Tutorial de Python: https://www.geeksforgeeks.org/python-programming-language/\n",
        "* PySpark Documentation: https://spark.apache.org/docs/latest/api/python/\n",
        "* RDD Programming Guide: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
        "* Introduction to Parallel Computing Tutorial: https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial"
      ],
      "metadata": {
        "id": "9vv6TS7cC5MP"
      }
    }
  ]
}